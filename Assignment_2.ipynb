{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7009fe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461c5a5e",
   "metadata": {},
   "source": [
    "**Load the Dataset as a pandas dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "612ada82",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_dataset = pd.read_csv('./Data/RAW_recipes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a604f2",
   "metadata": {},
   "source": [
    "Tags are stored as a list but because CSV files are comma separated, the list but be a **string** in order to not to conflict with other values. We want to convert that into a **list**. (Example 'Tag' = \"[American, sweet, desert, chocolate]\")\n",
    "   \n",
    "Steps, Ingredients, and Nutritional Information are stored the same as tags so we also want to convert them into **lists**\n",
    "\n",
    "Calories are always the first element in the nutritional information list, therefore make a column that contains only caloric information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fe7e0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_dataset['tags'] = recipes_dataset['tags'].apply(ast.literal_eval)\n",
    "recipes_dataset['steps'] = recipes_dataset['steps'].apply(ast.literal_eval)\n",
    "recipes_dataset['ingredients'] = recipes_dataset['ingredients'].apply(ast.literal_eval)\n",
    "recipes_dataset['nutrition'] = recipes_dataset['nutrition'].apply(ast.literal_eval)\n",
    "recipes_dataset['calories'] = recipes_dataset['nutrition'].apply(lambda x: x[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663723e3",
   "metadata": {},
   "source": [
    "Iterate through the dataframe and add each recipe to the **recipes dictionary**. This dictionary will hold all recipes and have the recipe id as the key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14b4cd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes = {}\n",
    "\n",
    "for index, row in recipes_dataset.iterrows():\n",
    "    recipes[row['id']] = {\n",
    "        'name' : row['name'],\n",
    "        'id' : row['id'],\n",
    "        'minutes' : int(row['minutes']),\n",
    "        'contributor_id' : row['contributor_id'],\n",
    "        'submitted' : pd.to_datetime(row['submitted']),\n",
    "        'tags' : set(row['tags']),\n",
    "        'nutrition' : row['nutrition'],\n",
    "        'n_steps' : int(row['n_steps']),\n",
    "        'steps' : set(row['steps']),\n",
    "        'description' : row['description'],\n",
    "        'ingredients' : set(row['ingredients']),\n",
    "        'n_ingredients' : int(row['n_ingredients']),\n",
    "        'calories' : int(row['calories']) \n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ef90318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'arriba   baked winter squash mexican style',\n",
       " 'id': 137739,\n",
       " 'minutes': 55,\n",
       " 'contributor_id': 47892,\n",
       " 'submitted': Timestamp('2005-09-16 00:00:00'),\n",
       " 'tags': {'60-minutes-or-less',\n",
       "  'christmas',\n",
       "  'course',\n",
       "  'cuisine',\n",
       "  'dietary',\n",
       "  'easy',\n",
       "  'fall',\n",
       "  'holiday-event',\n",
       "  'main-ingredient',\n",
       "  'mexican',\n",
       "  'north-american',\n",
       "  'occasion',\n",
       "  'preparation',\n",
       "  'seasonal',\n",
       "  'side-dishes',\n",
       "  'squash',\n",
       "  'time-to-make',\n",
       "  'vegetables',\n",
       "  'vegetarian',\n",
       "  'winter'},\n",
       " 'nutrition': [51.5, 0.0, 13.0, 0.0, 2.0, 0.0, 4.0],\n",
       " 'n_steps': 11,\n",
       " 'steps': {'bake at 350 degrees , again depending on size , for 40 minutes up to an hour , until a fork can easily pierce the skin',\n",
       "  'be careful not to burn the squash especially if you opt to use sugar or butter',\n",
       "  'depending on size of squash , cut into half or fourths',\n",
       "  'for spicy squash , drizzle olive oil or melted butter over each cut squash piece',\n",
       "  'for sweet squash , drizzle melted honey , butter , grated piloncillo over each cut squash piece',\n",
       "  'if desired , season with salt',\n",
       "  'if you feel more comfortable , cover the squash with aluminum foil the first half hour , give or take , of baking',\n",
       "  'make a choice and proceed with recipe',\n",
       "  'remove seeds',\n",
       "  'season with mexican seasoning mix ii',\n",
       "  'season with sweet mexican spice mix'},\n",
       " 'description': 'autumn is my favorite time of year to cook! this recipe \\r\\ncan be prepared either spicy or sweet, your choice!\\r\\ntwo of my posted mexican-inspired seasoning mix recipes are offered as suggestions.',\n",
       " 'ingredients': {'butter',\n",
       "  'honey',\n",
       "  'mexican seasoning',\n",
       "  'mixed spice',\n",
       "  'olive oil',\n",
       "  'salt',\n",
       "  'winter squash'},\n",
       " 'n_ingredients': 7,\n",
       " 'calories': 51}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test the recipe that is the first element in the dataframe using its id\n",
    "recipes[137739]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342c141c",
   "metadata": {},
   "source": [
    "Load all the reviews into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67163ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = pd.read_csv('./Data/RAW_interactions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1c44d8",
   "metadata": {},
   "source": [
    "Convert the dates to usable dates using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "982e4223",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions['date'] = pd.to_datetime(interactions['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e13b581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(s1,s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s2.union(s1))\n",
    "    return numer / denom if denom > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7b8d085",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8104bd",
   "metadata": {},
   "source": [
    "Create a list of tuples of user, item, and ratings to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a4315e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in interactions.iterrows():\n",
    "    user, item, rating = row['user_id'], row['recipe_id'], row['rating']\n",
    "    ratings.append((user,item,rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ebc029",
   "metadata": {},
   "source": [
    "Shuffle the ratings then create two sets. One will be used to train the data, the other will be used to check its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3c5730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(ratings)\n",
    "number_of_ratingss = len(ratings)\n",
    "split = int(0.8 * number_of_ratingss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1435d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = ratings[:split]\n",
    "validation_data = ratings[split:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af019622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recipeVector(recipe):\n",
    "    return [recipe.get('minutes',0), recipe.get('n_steps',0),recipe.get('calories',0), recipe.get('n_ingredients',0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "297db8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression MSE: 1.5961509420214919\n",
      "\n",
      "Feature Coefficients:\n",
      "  Minutes: 0.00000\n",
      "  N_Steps: -0.00501\n",
      "  Calories: -0.00001\n",
      "  N_Ingredients: 0.00205\n",
      "  Intercept (Base Rating): 4.44726\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for u, i, r in training_data:\n",
    "    if i in recipes:\n",
    "        X_train.append(recipeVector(recipes[i]))\n",
    "        y_train.append(r)\n",
    "\n",
    "X_valid = []\n",
    "y_valid = []\n",
    "\n",
    "for u, i, r in validation_data:\n",
    "    if i in recipes:\n",
    "        X_valid.append(recipeVector(recipes[i]))\n",
    "        y_valid.append(r)\n",
    "\n",
    "# 3. Train the Linear Regression Model\n",
    "# fit_intercept=True adds the bias term (theta_0/offset) automatically\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 4. Evaluate\n",
    "predictions = model.predict(np.array(X_valid))\n",
    "mse = np.mean((np.array(y_valid) - predictions) ** 2)\n",
    "\n",
    "print(f\"Linear Regression MSE: {mse}\")\n",
    "\n",
    "# 5. Analyze Coefficients\n",
    "# This tells you which features actually matter\n",
    "feature_names = ['Minutes', 'N_Steps', 'Calories', 'N_Ingredients']\n",
    "print(\"\\nFeature Coefficients:\")\n",
    "for name, coef in zip(feature_names, model.coef_):\n",
    "    print(f\"  {name}: {coef:.5f}\")\n",
    "print(f\"  Intercept (Base Rating): {model.intercept_:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57793bc3",
   "metadata": {},
   "source": [
    "Create dictionaries of each rating a user has given an item and each rating a recipe has received from each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1cd8371",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_per_user = defaultdict(list)\n",
    "reviews_per_item = defaultdict(list)\n",
    "for user, item, rating in training_data:\n",
    "    reviews_per_user[user].append((item, rating))\n",
    "    reviews_per_item[item].append((user, rating))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a02d4ee",
   "metadata": {},
   "source": [
    "Calculate the mean rating of all ratings in the training set and get the MSE using the global average as a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8951a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_avg_rating = np.mean([r for _, _, r in training_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbe8db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Average Baseline MSE: 1.596962598331192\n"
     ]
    }
   ],
   "source": [
    "global_avg_mse = 0.0\n",
    "for u, i, r in validation_data:\n",
    "    global_avg_mse += (r - global_avg_rating) ** 2\n",
    "global_avg_mse /= len(validation_data)\n",
    "\n",
    "print(f\"Global Average Baseline MSE: {global_avg_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3443174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alphaUpdate(ratingsTrain, alpha, betaU, betaI, lamb):\n",
    "    newAlpha = 0\n",
    "    for u,b,r in ratingsTrain:\n",
    "        newAlpha += r - (betaU[u] + betaI[b])\n",
    "    return newAlpha / len(ratingsTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "062e14ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def betaUUpdate(ratingsPerUser, alpha, betaU, betaI, lamb):\n",
    "    newBetaU = {}\n",
    "    for u in ratingsPerUser:\n",
    "        newBeta = 0\n",
    "        for b,r in ratingsPerUser[u]:\n",
    "            newBeta += r - (alpha + betaI[b])\n",
    "        newBetaU[u] = newBeta / (lamb + len(ratingsPerUser[u]))\n",
    "    return newBetaU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6e7bcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def betaIUpdate(ratingsPerItem, alpha, betaU, betaI, lamb):\n",
    "    newBetaI = {}\n",
    "    for b in ratingsPerItem:\n",
    "        newBeta = 0\n",
    "        for u,r in ratingsPerItem[b]:\n",
    "            newBeta += r - (alpha + betaU[u])\n",
    "        newBetaI[b] = newBeta / (lamb + len(ratingsPerItem[b]))\n",
    "    return newBetaI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f536dc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def userBiasModel(ratingsTrain, ratingsPerUser, ratingsPerItem, alpha, betaU, betaI,lamb):\n",
    "    for i in range(10):\n",
    "        alpha = alphaUpdate(ratingsTrain, alpha, betaU, betaI, lamb)\n",
    "        betaU = betaUUpdate(ratingsPerUser, alpha, betaU, betaI, lamb)\n",
    "        betaI = betaIUpdate(ratingsPerItem, alpha, betaU, betaI, lamb)\n",
    "    return alpha, betaU, betaI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51e021c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMSE(alpha,betaI,betaU, dataset):\n",
    "    sse = 0\n",
    "    for user, item, rating in dataset:\n",
    "        prediction = alpha + betaU.get(user, 0.0) + betaI.get(item, 0.0)\n",
    "        sse += (rating - prediction) ** 2\n",
    "    mse = sse / len(dataset)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c43d0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_review_history = defaultdict(list)\n",
    "\n",
    "interactions.sort_values(by=['user_id', 'date'], inplace=True)\n",
    "\n",
    "current_history = defaultdict(list)\n",
    "\n",
    "for index, row in interactions.iterrows():\n",
    "    user_id = int(row['user_id'])\n",
    "    review_text = str(row['review'])\n",
    "\n",
    "    user_review_history[user_id] = \" \".join(current_history[user_id])\n",
    "    \n",
    "    current_history[user_id].append(review_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df84b3b",
   "metadata": {},
   "source": [
    "Run a model that calculates the user's bias and use that for prediction. Find the best $\\lambda$ to minimize MSE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c571cc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.603127937867828\n",
      "500.0\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize Parameters\n",
    "alpha = global_avg_rating\n",
    "betaU = {u: 0.0 for u in reviews_per_user}\n",
    "betaI = {i: 0.0 for i in reviews_per_item}\n",
    "lambs = [0.1,0.5,1.0,5.0,500.0]\n",
    "\n",
    "bestMSE = 100.0\n",
    "bestIndex = 0\n",
    "for i in range(len(lambs)):\n",
    "    # 2. Run Training\n",
    "    lamb = lambs[i]\n",
    "    alpha = global_avg_rating\n",
    "    betaU = {u: 0.0 for u in reviews_per_user}\n",
    "    betaI = {i: 0.0 for i in reviews_per_item}\n",
    "\n",
    "    alpha, betaU, betaI = userBiasModel(training_data, reviews_per_user, reviews_per_item, alpha, betaU, betaI, lamb)\n",
    "    # 3. Evaluate\n",
    "    train_mse = getMSE(alpha, betaU, betaI, training_data)\n",
    "    valid_mse = getMSE(alpha, betaU, betaI, validation_data)\n",
    "\n",
    "    if bestMSE > valid_mse:\n",
    "        bestMSE = valid_mse\n",
    "        bestIndex = i\n",
    "\n",
    "print(bestMSE)\n",
    "print(lambs[bestIndex])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f104bb82",
   "metadata": {},
   "source": [
    "Create a dictionary: User ID -> Set of all ingredients they have used\n",
    "We only want to keep ingredients from recipes with ratings > 4 that way we are not adding disliked ingredients, same for tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "741437c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building user profiles...\n",
      "Built profiles for 153898 users\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary: User ID -> Set of all ingredients they have liked\n",
    "user_ingredient_profiles = defaultdict(set)\n",
    "user_tag_profiles = defaultdict(set)\n",
    "\n",
    "print(\"Building user profiles...\")\n",
    "for u, i, r in training_data:\n",
    "    # Only consider recipes they liked (e.g., rating >= 4)\n",
    "    # This reduces noise from things they hated\n",
    "    if r >= 4:\n",
    "        # Check if we have ingredient info for this recipe\n",
    "        if i in recipes:\n",
    "            recipe_ingredients = recipes[i]['ingredients']\n",
    "            user_ingredient_profiles[u].update(recipe_ingredients)\n",
    "            recipe_tags = recipes[i]['tags']\n",
    "            user_tag_profiles[u].update(recipe_tags)\n",
    "\n",
    "print(f\"Built profiles for {len(user_ingredient_profiles)} users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ce6b88",
   "metadata": {},
   "source": [
    "Calculate the rating based off the user's bias and the similarity of the recipe's tags and ingredients to the user's tags and ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d5d1914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarityBiasModel(u, i, alpha, betaU, betaI, weight1,weight2):\n",
    "    bias_pred = alpha + betaU.get(u, 0.0) + betaI.get(i, 0.0)\n",
    "    \n",
    "    jaccard_ingredients = 0.0\n",
    "    if u in user_ingredient_profiles and i in recipes:\n",
    "        u_ingredients = user_ingredient_profiles[u]\n",
    "        i_ingredients = recipes[i]['ingredients']\n",
    "         \n",
    "        # Jaccard Calculation\n",
    "        jaccard_ingredients = jaccard(u_ingredients,i_ingredients)\n",
    "\n",
    "    jaccard_tag = 0.0\n",
    "    if u in user_tag_profiles and i in recipes:\n",
    "        u_tags = user_tag_profiles[u]\n",
    "        i_tags = recipes[i]['tags']\n",
    "\n",
    "        jaccard_tag = jaccard(u_tags,i_tags)    \n",
    "    return bias_pred + (weight1 * (jaccard_tag)) +( weight2 * (jaccard_ingredients))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636c092e",
   "metadata": {},
   "source": [
    "Find the best weights of the ingredient and tag similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3a2571a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Hybrid Weight...\n",
      "Weight: (0.0,0.0), MSE: 1.5369389034777867\n",
      "Weight: (0.0,0.1), MSE: 1.5365632615387113\n",
      "Weight: (0.0,0.25), MSE: 1.536059928793883\n",
      "Weight: (0.0,0.5), MSE: 1.5353813879891642\n",
      "Weight: (0.0,0.75), MSE: 1.5349032810636798\n",
      "Weight: (0.0,1.0), MSE: 1.5346256080173282\n",
      "Weight: (0.1,0.0), MSE: 1.5353024960042652\n",
      "Weight: (0.1,0.1), MSE: 1.5349992586720598\n",
      "Weight: (0.1,0.25), MSE: 1.5346045328374274\n",
      "Weight: (0.1,0.5), MSE: 1.5341070035496294\n",
      "Weight: (0.1,0.75), MSE: 1.533809908141143\n",
      "Weight: (0.1,1.0), MSE: 1.5337132466118415\n",
      "Weight: (0.25,0.0), MSE: 1.5334795843496263\n",
      "Weight: (0.25,0.1), MSE: 1.5332849539275557\n",
      "Weight: (0.25,0.25), MSE: 1.5330531384582529\n",
      "Weight: (0.25,0.5), MSE: 1.5328271264460116\n",
      "Weight: (0.25,0.75), MSE: 1.5328015483130213\n",
      "Weight: (0.25,1.0), MSE: 1.5329764040591463\n",
      "Weight: (0.5,0.0), MSE: 1.5321259304066923\n",
      "Weight: (0.5,0.1), MSE: 1.5321123115016597\n",
      "Weight: (0.5,0.25), MSE: 1.5321520133078355\n",
      "Weight: (0.5,0.5), MSE: 1.5323785300881076\n",
      "Weight: (0.5,0.75), MSE: 1.5328054807475464\n",
      "Weight: (0.5,1.0), MSE: 1.5334328652862088\n",
      "Weight: (0.75,0.0), MSE: 1.5328779416490315\n",
      "Weight: (0.75,0.1), MSE: 1.533045334260916\n",
      "Weight: (0.75,0.25), MSE: 1.533356553342574\n",
      "Weight: (0.75,0.5), MSE: 1.5340355989153844\n",
      "Weight: (0.75,0.75), MSE: 1.5349150783673422\n",
      "Weight: (0.75,1.0), MSE: 1.535994991698462\n",
      "Weight: (1.0,0.0), MSE: 1.5357356180765476\n",
      "Weight: (1.0,0.1), MSE: 1.5360840222054701\n",
      "Weight: (1.0,0.25), MSE: 1.536666758562625\n",
      "Weight: (1.0,0.5), MSE: 1.537798332927865\n",
      "Weight: (1.0,0.75), MSE: 1.5391303411723245\n",
      "Weight: (1.0,1.0), MSE: 1.5406627832959277\n",
      "\n",
      "Best Hybrid MSE: 1.5321123115016597 (at weight 0.5,0.1)\n"
     ]
    }
   ],
   "source": [
    "# Use the best parameters from your previous tuning (e.g., Lambda=10 or 100)\n",
    "# alpha, betaU, betaI = ... (your trained values)\n",
    "\n",
    "weights = [0.0,0.1,0.25,0.5,0.75,1.0]\n",
    "best_hybrid_mse = float('inf')\n",
    "best_weight = 0\n",
    "best_weight2 = 0\n",
    "best_pen = 0\n",
    "print(\"Tuning Hybrid Weight...\")\n",
    "for w in weights:\n",
    "    for j in weights:\n",
    "        sse = 0\n",
    "        for u, i, r in validation_data:\n",
    "            pred = similarityBiasModel(u, i, alpha, betaU, betaI, w,j)\n",
    "            sse += (r - pred) ** 2\n",
    "        \n",
    "        mse = sse / len(validation_data)\n",
    "        print(f\"Weight: ({w},{j}), MSE: {mse}\")\n",
    "        \n",
    "        if mse < best_hybrid_mse:\n",
    "            best_hybrid_mse = mse\n",
    "            best_weight = w\n",
    "            best_weight2 = j\n",
    "\n",
    "print(f\"\\nBest Hybrid MSE: {best_hybrid_mse} (at weight {best_weight},{best_weight2})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb1e410b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing improvements...\n",
      "User 774494 rated Item 257276 as 5\n",
      "  Bias Prediction: 4.27 (Error: 0.73)\n",
      "  Hybrid Prediction: 4.98 (Error: 0.02)\n",
      "  Common Ingredients: ['sour cream', 'elbow macaroni', 'eggs', 'black pepper', 'sharp cheddar cheese']\n"
     ]
    }
   ],
   "source": [
    "# Find a case where Hybrid beat the Bias model\n",
    "print(\"analyzing improvements...\")\n",
    "for u, i, r in validation_data:\n",
    "    # 1. Calculate Bias Prediction\n",
    "    bias_pred = alpha + betaU.get(u, 0.0) + betaI.get(i, 0.0)\n",
    "    \n",
    "    # 2. Calculate Hybrid Prediction\n",
    "    bias_sim_pred = similarityBiasModel(u, i, alpha, betaU, betaI, 0.5,0.25)\n",
    "    \n",
    "    # 3. Check error\n",
    "    err_bias = abs(r - bias_pred)\n",
    "    err_hybrid = abs(r - bias_sim_pred)\n",
    "    \n",
    "    # Did Hybrid win by a lot? (e.g., improved by 0.5 stars)\n",
    "    if err_hybrid < err_bias - 0.5:\n",
    "        print(f\"User {u} rated Item {i} as {r}\")\n",
    "        print(f\"  Bias Prediction: {bias_pred:.2f} (Error: {err_bias:.2f})\")\n",
    "        print(f\"  Hybrid Prediction: {bias_sim_pred:.2f} (Error: {err_hybrid:.2f})\")\n",
    "        \n",
    "        # Print ingredients to explain WHY\n",
    "        if u in user_ingredient_profiles and i in recipes:\n",
    "            common = user_ingredient_profiles[u].intersection(recipes[i]['ingredients'])\n",
    "            print(f\"  Common Ingredients: {list(common)[:5]}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3a853f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the Pre-Processed Data\n",
    "pp_recipes = pd.read_csv('./Data/PP_recipes.csv')\n",
    "pp_users = pd.read_csv('./Data/PP_users.csv')\n",
    "\n",
    "# 2. Parse the list columns (Strings -> Lists)\n",
    "# This converts \"[1, 2, 3]\" to the list [1, 2, 3]\n",
    "pp_recipes['ingredient_ids'] = pp_recipes['ingredient_ids'].apply(ast.literal_eval)\n",
    "pp_users['items'] = pp_users['items'].apply(ast.literal_eval)\n",
    "pp_users['ratings'] = pp_users['ratings'].apply(ast.literal_eval)\n",
    "\n",
    "\n",
    "# 3. Create Training Samples (Flattening the user history)\n",
    "# The PP_users file has one row per user. We need to \"explode\" this \n",
    "# so we have one row per interaction (User, Item, Rating) for training.\n",
    "\n",
    "interaction_list = []\n",
    "\n",
    "for index, row in pp_users.iterrows():\n",
    "    user_internal_id = row['u']\n",
    "    item_list = row['items']\n",
    "    rating_list = row['ratings']\n",
    "    \n",
    "    # Zip them together to get individual interactions\n",
    "    for item_id, rating in zip(item_list, rating_list):\n",
    "        interaction_list.append([user_internal_id, item_id, rating])\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_train = pd.DataFrame(interaction_list, columns=['u', 'i', 'rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66360c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = train_test_split(df_train, test_size=0.2, random_state=42)\n",
    "\n",
    "class FoodDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.users = torch.tensor(df['u'].values, dtype=torch.long)\n",
    "        self.items = torch.tensor(df['i'].values, dtype=torch.long)\n",
    "        self.ratings = torch.tensor(df['rating'].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.items[idx], self.ratings[idx]\n",
    "\n",
    "train_dataset = FoodDataset(train_df)\n",
    "valid_dataset = FoodDataset(valid_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3db3f3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorizationNN(nn.Module):\n",
    "    def __init__(self, num_users, num_items, n_factors=8):\n",
    "        super(MatrixFactorizationNN, self).__init__()\n",
    "        \n",
    "        # User Embeddings (Gamma_u) & Bias (Beta_u)\n",
    "        self.user_factors = nn.Embedding(num_users, n_factors)\n",
    "        self.user_bias = nn.Embedding(num_users, 1)\n",
    "        \n",
    "        # Item Embeddings (Gamma_i) & Bias (Beta_i)\n",
    "        self.item_factors = nn.Embedding(num_items, n_factors)\n",
    "        self.item_bias = nn.Embedding(num_items, 1)\n",
    "        \n",
    "        # Global Bias (Alpha) - a single learnable number\n",
    "        self.global_bias = nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        # 1. Get Embeddings\n",
    "        u_factor = self.user_factors(user)\n",
    "        i_factor = self.item_factors(item)\n",
    "        \n",
    "        # 2. Dot Product (Similarity)\n",
    "        # (Batch_size, Factors) * (Batch_size, Factors) -> Sum -> (Batch_size, 1)\n",
    "        dot_product = (u_factor * i_factor).sum(dim=1, keepdim=True)\n",
    "        \n",
    "        # 3. Add Biases\n",
    "        u_b = self.user_bias(user)\n",
    "        i_b = self.item_bias(item)\n",
    "        \n",
    "        # Prediction = Alpha + Beta_u + Beta_i + (Gamma_u * Gamma_i)\n",
    "        output = self.global_bias + u_b + i_b + dot_product\n",
    "        \n",
    "        return output.squeeze() # Remove extra dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9337873e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Neural Training...\n",
      "Epoch 1: Train MSE = 1.8243, Valid MSE = 0.8462\n",
      "Epoch 2: Train MSE = 0.8460, Valid MSE = 0.8469\n",
      "Epoch 3: Train MSE = 0.8463, Valid MSE = 0.8485\n",
      "Epoch 4: Train MSE = 0.8452, Valid MSE = 0.8469\n",
      "Epoch 5: Train MSE = 0.8466, Valid MSE = 0.8476\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize Model\n",
    "# We need the max ID to know how many embeddings to create\n",
    "num_users = df_train['u'].max() + 1\n",
    "num_items = df_train['i'].max() + 1\n",
    "\n",
    "model = MatrixFactorizationNN(num_users, num_items, n_factors=4) # 8 latent factors\n",
    "criterion = nn.MSELoss() # We want to minimize Mean Squared Error\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "# 2. Training Loop\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"Starting Neural Training...\")\n",
    "for epoch in range(5): # Run for 5 epochs\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for users, items, ratings in train_loader:\n",
    "        users, items, ratings = users.to(device), items.to(device), ratings.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(users, items)\n",
    "        loss = criterion(predictions, ratings)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    # Validation Step\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for users, items, ratings in valid_loader:\n",
    "            users, items, ratings = users.to(device), items.to(device), ratings.to(device)\n",
    "            preds = model(users, items)\n",
    "            v_loss = criterion(preds, ratings)\n",
    "            val_loss += v_loss.item()\n",
    "            \n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(valid_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Train MSE = {avg_train_loss:.4f}, Valid MSE = {avg_val_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
