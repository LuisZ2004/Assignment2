{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7009fe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import ast\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "612ada82",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_dataset = pd.read_csv('./Data/RAW_recipes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fe7e0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_dataset['tags'] = recipes_dataset['tags'].apply(ast.literal_eval)\n",
    "recipes_dataset['steps'] = recipes_dataset['steps'].apply(ast.literal_eval)\n",
    "recipes_dataset['ingredients'] = recipes_dataset['ingredients'].apply(ast.literal_eval)\n",
    "recipes_dataset['nutrition'] = recipes_dataset['nutrition'].apply(ast.literal_eval)\n",
    "recipes_dataset['calories'] = recipes_dataset['nutrition'].apply(lambda x: x[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14b4cd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes = {}\n",
    "\n",
    "for index, row in recipes_dataset.iterrows():\n",
    "    recipes[row['id']] = {\n",
    "        'name' : row['name'],\n",
    "        'id' : row['id'],\n",
    "        'minutes' : int(row['minutes']),\n",
    "        'contributor_id' : row['contributor_id'],\n",
    "        'submitted' : pd.to_datetime(row['submitted']),\n",
    "        'tags' : set(row['tags']),\n",
    "        'nutrition' : row['nutrition'],\n",
    "        'n_steps' : int(row['n_steps']),\n",
    "        'steps' : set(row['steps']),\n",
    "        'description' : row['description'],\n",
    "        'ingredients' : set(row['ingredients']),\n",
    "        'n_ingredients' : int(row['n_ingredients']),\n",
    "        'calories' : int(row['calories']) \n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ef90318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'arriba   baked winter squash mexican style',\n",
       " 'id': 137739,\n",
       " 'minutes': 55,\n",
       " 'contributor_id': 47892,\n",
       " 'submitted': Timestamp('2005-09-16 00:00:00'),\n",
       " 'tags': {'60-minutes-or-less',\n",
       "  'christmas',\n",
       "  'course',\n",
       "  'cuisine',\n",
       "  'dietary',\n",
       "  'easy',\n",
       "  'fall',\n",
       "  'holiday-event',\n",
       "  'main-ingredient',\n",
       "  'mexican',\n",
       "  'north-american',\n",
       "  'occasion',\n",
       "  'preparation',\n",
       "  'seasonal',\n",
       "  'side-dishes',\n",
       "  'squash',\n",
       "  'time-to-make',\n",
       "  'vegetables',\n",
       "  'vegetarian',\n",
       "  'winter'},\n",
       " 'nutrition': [51.5, 0.0, 13.0, 0.0, 2.0, 0.0, 4.0],\n",
       " 'n_steps': 11,\n",
       " 'steps': {'bake at 350 degrees , again depending on size , for 40 minutes up to an hour , until a fork can easily pierce the skin',\n",
       "  'be careful not to burn the squash especially if you opt to use sugar or butter',\n",
       "  'depending on size of squash , cut into half or fourths',\n",
       "  'for spicy squash , drizzle olive oil or melted butter over each cut squash piece',\n",
       "  'for sweet squash , drizzle melted honey , butter , grated piloncillo over each cut squash piece',\n",
       "  'if desired , season with salt',\n",
       "  'if you feel more comfortable , cover the squash with aluminum foil the first half hour , give or take , of baking',\n",
       "  'make a choice and proceed with recipe',\n",
       "  'remove seeds',\n",
       "  'season with mexican seasoning mix ii',\n",
       "  'season with sweet mexican spice mix'},\n",
       " 'description': 'autumn is my favorite time of year to cook! this recipe \\r\\ncan be prepared either spicy or sweet, your choice!\\r\\ntwo of my posted mexican-inspired seasoning mix recipes are offered as suggestions.',\n",
       " 'ingredients': {'butter',\n",
       "  'honey',\n",
       "  'mexican seasoning',\n",
       "  'mixed spice',\n",
       "  'olive oil',\n",
       "  'salt',\n",
       "  'winter squash'},\n",
       " 'n_ingredients': 7,\n",
       " 'calories': 51}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes[137739]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67163ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = pd.read_csv('./Data/RAW_interactions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "982e4223",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions['date'] = pd.to_datetime(interactions['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e13b581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(s1,s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s2.union(s1))\n",
    "    return numer / denom if denom > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7b8d085",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_per_user = defaultdict(list)\n",
    "reviews_per_item = defaultdict(list)\n",
    "ratings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a4315e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in interactions.iterrows():\n",
    "    user, item, rating = row['user_id'], row['recipe_id'], row['rating']\n",
    "    reviews_per_user[user].append((item,rating))\n",
    "    reviews_per_item[item].append((user,rating))\n",
    "    ratings.append((user,item,rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3c5730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(ratings)\n",
    "number_of_ratingss = len(ratings)\n",
    "split = int(0.8 * number_of_ratingss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1435d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = ratings[:split]\n",
    "validation_data = ratings[split:]\n",
    "reviews_per_user = defaultdict(list)\n",
    "reviews_per_item = defaultdict(list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1cd8371",
   "metadata": {},
   "outputs": [],
   "source": [
    "for user, item, rating in training_data:\n",
    "    reviews_per_user[user].append((item, rating))\n",
    "    reviews_per_item[item].append((user, rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8951a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_avg_rating = np.mean([r for _, _, r in training_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3443174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alphaUpdate(ratingsTrain, alpha, betaU, betaI, lamb):\n",
    "    newAlpha = 0\n",
    "    for u,b,r in ratingsTrain:\n",
    "        newAlpha += r - (betaU[u] + betaI[b])\n",
    "    return newAlpha / len(ratingsTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "062e14ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def betaUUpdate(ratingsPerUser, alpha, betaU, betaI, lamb):\n",
    "    newBetaU = {}\n",
    "    for u in ratingsPerUser:\n",
    "        newBeta = 0\n",
    "        for b,r in ratingsPerUser[u]:\n",
    "            newBeta += r - (alpha + betaI[b])\n",
    "        newBetaU[u] = newBeta / (lamb + len(ratingsPerUser[u]))\n",
    "    return newBetaU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f6e7bcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def betaIUpdate(ratingsPerItem, alpha, betaU, betaI, lamb):\n",
    "    newBetaI = {}\n",
    "    for b in ratingsPerItem:\n",
    "        newBeta = 0\n",
    "        for u,r in ratingsPerItem[b]:\n",
    "            newBeta += r - (alpha + betaU[u])\n",
    "        newBetaI[b] = newBeta / (lamb + len(ratingsPerItem[b]))\n",
    "    return newBetaI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f536dc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def goodModel(ratingsTrain, ratingsPerUser, ratingsPerItem, alpha, betaU, betaI,lamb):\n",
    "    for i in range(20):\n",
    "        alpha = alphaUpdate(ratingsTrain, alpha, betaU, betaI, lamb)\n",
    "        betaU = betaUUpdate(ratingsPerUser, alpha, betaU, betaI, lamb)\n",
    "        betaI = betaIUpdate(ratingsPerItem, alpha, betaU, betaI, lamb)\n",
    "    return alpha, betaU, betaI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51e021c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMSE(alpha,betaI,betaU, dataset):\n",
    "    sse = 0\n",
    "    for user, item, rating in dataset:\n",
    "        prediction = alpha + betaU.get(user, 0.0) + betaI.get(item, 0.0)\n",
    "        sse += (rating - prediction) ** 2\n",
    "    mse = sse / len(dataset)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c43d0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_review_history = defaultdict(list)\n",
    "\n",
    "interactions.sort_values(by=['user_id', 'date'], inplace=True)\n",
    "\n",
    "current_history = defaultdict(list)\n",
    "\n",
    "for index, row in interactions.iterrows():\n",
    "    user_id = int(row['user_id'])\n",
    "    review_text = str(row['review'])\n",
    "\n",
    "    user_review_history[user_id] = \" \".join(current_history[user_id])\n",
    "    \n",
    "    current_history[user_id].append(review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c571cc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6099008844793672\n",
      "500.0\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize Parameters\n",
    "alpha = global_avg_rating\n",
    "betaU = {u: 0.0 for u in reviews_per_user}\n",
    "betaI = {i: 0.0 for i in reviews_per_item}\n",
    "lambs = [100.0,500.0,200.0,150.0]\n",
    "\n",
    "bestMSE = 100.0\n",
    "bestIndex = 0\n",
    "for i in range(len(lambs)):\n",
    "    # 2. Run Training\n",
    "    lamb = lambs[i]\n",
    "    alpha = global_avg_rating\n",
    "    betaU = {u: 0.0 for u in reviews_per_user}\n",
    "    betaI = {i: 0.0 for i in reviews_per_item}\n",
    "\n",
    "    alpha, betaU, betaI = goodModel(training_data, reviews_per_user, reviews_per_item, alpha, betaU, betaI, lamb)\n",
    "    # 3. Evaluate\n",
    "    train_mse = getMSE(alpha, betaU, betaI, training_data)\n",
    "    valid_mse = getMSE(alpha, betaU, betaI, validation_data)\n",
    "\n",
    "    if bestMSE > valid_mse:\n",
    "        bestMSE = valid_mse\n",
    "        bestIndex = i\n",
    "\n",
    "print(bestMSE)\n",
    "print(lambs[bestIndex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7af36da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Average Baseline MSE: 1.6040004469551954\n"
     ]
    }
   ],
   "source": [
    "# Calculate the MSE of just guessing the global average for everyone\n",
    "global_avg_mse = 0\n",
    "for u, i, r in validation_data:\n",
    "    global_avg_mse += (r - global_avg_rating) ** 2\n",
    "global_avg_mse /= len(validation_data)\n",
    "\n",
    "print(f\"Global Average Baseline MSE: {global_avg_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a6fd62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictHybrid(u, i, alpha, betaU, betaI, weight=1.0):\n",
    "    # 1. Base Prediction (from your existing model)\n",
    "    bias_pred = alpha + betaU.get(u, 0.0) + betaI.get(i, 0.0)\n",
    "    \n",
    "    # 2. Content Bonus (Jaccard)\n",
    "    jaccard_score = 0.0\n",
    "    if u in user_ingredient_profiles and i in recipes:\n",
    "        u_ingredients = user_ingredient_profiles[u]\n",
    "        i_ingredients = recipes[i]['ingredients']\n",
    "        \n",
    "        # Jaccard Calculation\n",
    "        intersection = len(u_ingredients.intersection(i_ingredients))\n",
    "        union = len(u_ingredients.union(i_ingredients))\n",
    "        if union > 0:\n",
    "            jaccard_score = intersection / union\n",
    "            \n",
    "    # Combine them\n",
    "    return bias_pred + (weight * jaccard_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "741437c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building user profiles...\n",
      "Built profiles for 153939 users\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary: User ID -> Set of all ingredients they have liked\n",
    "user_ingredient_profiles = defaultdict(set)\n",
    "\n",
    "print(\"Building user profiles...\")\n",
    "for u, i, r in training_data:\n",
    "    # Only consider recipes they liked (e.g., rating >= 4)\n",
    "    # This reduces noise from things they hated\n",
    "    if r >= 4:\n",
    "        # Check if we have ingredient info for this recipe\n",
    "        if i in recipes:\n",
    "            recipe_ingredients = recipes[i]['ingredients']\n",
    "            user_ingredient_profiles[u].update(recipe_ingredients)\n",
    "\n",
    "print(f\"Built profiles for {len(user_ingredient_profiles)} users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3a2571a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Hybrid Weight...\n",
      "Weight: 1.25, MSE: 1.5043767105734847\n",
      "Weight: 1.5, MSE: 1.504235601373332\n",
      "Weight: 1.75, MSE: 1.5042930981866565\n",
      "\n",
      "Best Hybrid MSE: 1.504235601373332 (at weight 1.5)\n"
     ]
    }
   ],
   "source": [
    "# Use the best parameters from your previous tuning (e.g., Lambda=10 or 100)\n",
    "# alpha, betaU, betaI = ... (your trained values)\n",
    "\n",
    "weights = [1.25, 1.5, 1.75]\n",
    "best_hybrid_mse = float('inf')\n",
    "best_weight = 0\n",
    "\n",
    "print(\"Tuning Hybrid Weight...\")\n",
    "for w in weights:\n",
    "    sse = 0\n",
    "    for u, i, r in validation_data:\n",
    "        pred = predictHybrid(u, i, alpha, betaU, betaI, weight=w)\n",
    "        sse += (r - pred) ** 2\n",
    "    \n",
    "    mse = sse / len(validation_data)\n",
    "    print(f\"Weight: {w}, MSE: {mse}\")\n",
    "    \n",
    "    if mse < best_hybrid_mse:\n",
    "        best_hybrid_mse = mse\n",
    "        best_weight = w\n",
    "\n",
    "print(f\"\\nBest Hybrid MSE: {best_hybrid_mse} (at weight {best_weight})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb1e410b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyzing improvements...\n",
      "User 1805530 rated Item 384181 as 5\n",
      "  Bias Prediction: 4.23 (Error: 0.77)\n",
      "  Hybrid Prediction: 4.83 (Error: 0.17)\n",
      "  Common Ingredients: ['salt', 'vegetable oil', 'flour', 'sugar', 'vanilla extract']\n"
     ]
    }
   ],
   "source": [
    "# Find a case where Hybrid beat the Bias model\n",
    "print(\"analyzing improvements...\")\n",
    "for u, i, r in validation_data:\n",
    "    # 1. Calculate Bias Prediction\n",
    "    bias_pred = alpha + betaU.get(u, 0.0) + betaI.get(i, 0.0)\n",
    "    \n",
    "    # 2. Calculate Hybrid Prediction\n",
    "    hybrid_pred = predictHybrid(u, i, alpha, betaU, betaI, weight=1.5)\n",
    "    \n",
    "    # 3. Check error\n",
    "    err_bias = abs(r - bias_pred)\n",
    "    err_hybrid = abs(r - hybrid_pred)\n",
    "    \n",
    "    # Did Hybrid win by a lot? (e.g., improved by 0.5 stars)\n",
    "    if err_hybrid < err_bias - 0.5:\n",
    "        print(f\"User {u} rated Item {i} as {r}\")\n",
    "        print(f\"  Bias Prediction: {bias_pred:.2f} (Error: {err_bias:.2f})\")\n",
    "        print(f\"  Hybrid Prediction: {hybrid_pred:.2f} (Error: {err_hybrid:.2f})\")\n",
    "        \n",
    "        # Print ingredients to explain WHY\n",
    "        if u in user_ingredient_profiles and i in recipes:\n",
    "            common = user_ingredient_profiles[u].intersection(recipes[i]['ingredients'])\n",
    "            print(f\"  Common Ingredients: {list(common)[:5]}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b3a853f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   u       i  rating\n",
      "0  0    1118     5.0\n",
      "1  0   27680     5.0\n",
      "2  0   32541     5.0\n",
      "3  0  137353     5.0\n",
      "4  0   16428     5.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 1. Load the Pre-Processed Data\n",
    "pp_recipes = pd.read_csv('./Data/PP_recipes.csv')\n",
    "pp_users = pd.read_csv('./Data/PP_users.csv')\n",
    "\n",
    "# 2. Parse the list columns (Strings -> Lists)\n",
    "# This converts \"[1, 2, 3]\" to the list [1, 2, 3]\n",
    "pp_recipes['ingredient_ids'] = pp_recipes['ingredient_ids'].apply(ast.literal_eval)\n",
    "pp_users['items'] = pp_users['items'].apply(ast.literal_eval)\n",
    "pp_users['ratings'] = pp_users['ratings'].apply(ast.literal_eval)\n",
    "\n",
    "# 3. Create Training Samples (Flattening the user history)\n",
    "# The PP_users file has one row per user. We need to \"explode\" this \n",
    "# so we have one row per interaction (User, Item, Rating) for training.\n",
    "\n",
    "interaction_list = []\n",
    "\n",
    "for index, row in pp_users.iterrows():\n",
    "    user_internal_id = row['u']\n",
    "    item_list = row['items']\n",
    "    rating_list = row['ratings']\n",
    "    \n",
    "    # Zip them together to get individual interactions\n",
    "    for item_id, rating in zip(item_list, rating_list):\n",
    "        interaction_list.append([user_internal_id, item_id, rating])\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_train = pd.DataFrame(interaction_list, columns=['u', 'i', 'rating'])\n",
    "\n",
    "print(df_train.head())\n",
    "# Output will look like:\n",
    "#    u      i     rating\n",
    "#    0    1118     5.0\n",
    "#    0    27680    5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "66360c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Split the data (80% train, 20% validation)\n",
    "train_df, valid_df = train_test_split(df_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Define the PyTorch Dataset\n",
    "class FoodDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.users = torch.tensor(df['u'].values, dtype=torch.long)\n",
    "        self.items = torch.tensor(df['i'].values, dtype=torch.long)\n",
    "        self.ratings = torch.tensor(df['rating'].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.users[idx], self.items[idx], self.ratings[idx]\n",
    "\n",
    "# 3. Create DataLoaders (Batching)\n",
    "train_dataset = FoodDataset(train_df)\n",
    "valid_dataset = FoodDataset(valid_df)\n",
    "\n",
    "# Batch size of 64 or 128 is standard\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3db3f3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorizationNN(nn.Module):\n",
    "    def __init__(self, num_users, num_items, n_factors=8):\n",
    "        super(MatrixFactorizationNN, self).__init__()\n",
    "        \n",
    "        # User Embeddings (Gamma_u) & Bias (Beta_u)\n",
    "        self.user_factors = nn.Embedding(num_users, n_factors)\n",
    "        self.user_bias = nn.Embedding(num_users, 1)\n",
    "        \n",
    "        # Item Embeddings (Gamma_i) & Bias (Beta_i)\n",
    "        self.item_factors = nn.Embedding(num_items, n_factors)\n",
    "        self.item_bias = nn.Embedding(num_items, 1)\n",
    "        \n",
    "        # Global Bias (Alpha) - a single learnable number\n",
    "        self.global_bias = nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "    def forward(self, user, item):\n",
    "        # 1. Get Embeddings\n",
    "        u_factor = self.user_factors(user)\n",
    "        i_factor = self.item_factors(item)\n",
    "        \n",
    "        # 2. Dot Product (Similarity)\n",
    "        # (Batch_size, Factors) * (Batch_size, Factors) -> Sum -> (Batch_size, 1)\n",
    "        dot_product = (u_factor * i_factor).sum(dim=1, keepdim=True)\n",
    "        \n",
    "        # 3. Add Biases\n",
    "        u_b = self.user_bias(user)\n",
    "        i_b = self.item_bias(item)\n",
    "        \n",
    "        # Prediction = Alpha + Beta_u + Beta_i + (Gamma_u * Gamma_i)\n",
    "        output = self.global_bias + u_b + i_b + dot_product\n",
    "        \n",
    "        return output.squeeze() # Remove extra dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9337873e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Neural Training...\n",
      "Epoch 1: Train MSE = 1.8997, Valid MSE = 0.8472\n",
      "Epoch 2: Train MSE = 0.8457, Valid MSE = 0.8468\n",
      "Epoch 3: Train MSE = 0.8462, Valid MSE = 0.8458\n",
      "Epoch 4: Train MSE = 0.8460, Valid MSE = 0.8472\n",
      "Epoch 5: Train MSE = 0.8460, Valid MSE = 0.8488\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize Model\n",
    "# We need the max ID to know how many embeddings to create\n",
    "num_users = df_train['u'].max() + 1\n",
    "num_items = df_train['i'].max() + 1\n",
    "\n",
    "model = MatrixFactorizationNN(num_users, num_items, n_factors=8) # 8 latent factors\n",
    "criterion = nn.MSELoss() # We want to minimize Mean Squared Error\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "# 2. Training Loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"Starting Neural Training...\")\n",
    "for epoch in range(5): # Run for 5 epochs\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for users, items, ratings in train_loader:\n",
    "        users, items, ratings = users.to(device), items.to(device), ratings.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(users, items)\n",
    "        loss = criterion(predictions, ratings)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    # Validation Step\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for users, items, ratings in valid_loader:\n",
    "            users, items, ratings = users.to(device), items.to(device), ratings.to(device)\n",
    "            preds = model(users, items)\n",
    "            v_loss = criterion(preds, ratings)\n",
    "            val_loss += v_loss.item()\n",
    "            \n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(valid_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Train MSE = {avg_train_loss:.4f}, Valid MSE = {avg_val_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
